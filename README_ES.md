# Cargador GGUF para Modelos Wan V1.0

Un cargador de alto rendimiento para modelos de generaci√≥n de v√≠deo Wan en formato 
`.gguf`, dise√±ado para reducir dr√°sticamente el consumo de memoria VRAM y RAM durante 
la inferencia.

## √çndice

- [¬øQu√© es gguf_loader?](#qu√©-es-gguf_loader)
- [Caracter√≠sticas Principales](#caracter√≠sticas-principales)
- [Rendimiento](#rendimiento)
- [Instalaci√≥n](#instalaci√≥n)
- [Uso R√°pido: Integraci√≥n en 3 Pasos](#uso-r√°pido-integraci√≥n-en-3-pasos)
- [Uso Avanzado: Control Granular](#uso-avanzado-control-granular)
- [Modelos GGUF Compatibles](#modelos-gguf-compatibles)
- [Preguntas Frecuentes](#preguntas-frecuentes)
- [Licencia](#licencia)
- [Agradecimientos](#agradecimientos)

## ¬øQu√© es gguf_loader?

Los modelos de generaci√≥n de v√≠deo como la familia Wan son extremadamente potentes, pero 
su gran tama√±o (especialmente el componente Transformer) exige una cantidad muy elevada 
de VRAM, limitando su uso a hardware de gama alta.

Este m√≥dulo soluciona ese problema implementando una **estrategia de decuantizaci√≥n 
sobre la marcha**. Permite cargar los pesos del Transformer desde un archivo `.gguf` 
cuantizado (4/5/8 bits), manteniendo el modelo en un formato de bajo consumo en memoria 
y decodificando √∫nicamente las capas necesarias en la VRAM en el momento preciso de su 
ejecuci√≥n.

El resultado es una **reducci√≥n dr√°stica del pico de memoria VRAM** requerida, haciendo 
posible ejecutar estos modelos en GPUs de consumo.

## Caracter√≠sticas Principales

- ‚ö° **Reducci√≥n Masiva de Memoria**: Reduce el consumo de VRAM del Transformer en un 
  70% o m√°s, dependiendo del nivel de cuantizaci√≥n
- üéÆ **Inferencia en GPUs de Consumo**: Permite la ejecuci√≥n de modelos como Wan-14B 
  en tarjetas gr√°ficas con VRAM limitada (ej. 12-16 GB)
- üîå **Integraci√≥n Sencilla**: Dise√±ado para reemplazar la carga est√°ndar de 
  `.safetensors` con una sola funci√≥n
- üß† **Gesti√≥n Inteligente de Memoria**: Incluye scripts que demuestran c√≥mo cargar 
  componentes de forma secuencial para minimizar la huella de memoria
- üõ†Ô∏è **Flexible**: Ofrece tanto carga "todo en uno" como funciones granulares para 
  control total

## Rendimiento

Con la inferencia de modelos GGUF he conseguido ejecutar, en una tarjeta **RTX 3090**, 
una carga ultra r√°pida en **20 segundos** del Transformer de 14B de Wan 2.1, 
incluyendo todos los componentes (tokenizer, VAE, etc.), y una inferencia de v√≠deo con 
las siguientes caracter√≠sticas en **40 segundos**:

- **Resoluci√≥n**: 512x320 p√≠xeles
- **Frames**: 81 frames
- **Pasos**: 4 steps
- **Modelo**: LightX2V destilado en formato GGUF

### Ventajas de GGUF

La principal ventaja de cargar modelos en formato GGUF es la **alta velocidad de carga**, 
aunque se pierda ligeramente algo de definici√≥n en comparaci√≥n con los modelos 
originales sin cuantizar.

## Instalaci√≥n

### Requisitos Previos

- Python 3.8 o superior
- Un proyecto que actualmente cargue el modelo Wan utilizando `diffusers`
- Un archivo de modelo Wan en formato `.gguf`

### Pasos de Instalaci√≥n

1. **Clona el repositorio:**
   ```bash
   git clone https://github.com/acaimari/gguf_loader.git
   cd gguf_loader
   ```

2. **Instala las dependencias:**
   ```bash
   pip install torch diffusers transformers imageio omegaconf-py
   pip install gguf
   ```

## Uso R√°pido: Integraci√≥n en 3 Pasos

Esta gu√≠a asume que ya tienes un proyecto funcional que carga un modelo Wan con diffusers.

### 1. Copia el M√≥dulo a tu Proyecto

Copia el directorio completo `gguf_loader/` en la ra√≠z de tu proyecto. Aseg√∫rate de 
mantener la estructura de carpetas para evitar errores de importaci√≥n.

### 2. Localiza tu C√≥digo de Carga Actual

Busca en tu c√≥digo la parte donde ensamblas el pipeline. Probablemente se parezca a esto:

**C√≥digo ANTERIOR (cargando con .safetensors):**
```python
from diffusers import WanPipeline, WanTransformer3DModel, AutoencoderKL

# ... Carga de VAE, Tokenizer, Scheduler ...

print("Cargando Transformer desde safetensors...")
transformer = WanTransformer3DModel.from_pretrained(
    "Wan-AI/Wan2.1-T2V-14B-Diffusers",
    subfolder="transformer"
)

pipe = WanPipeline(
    transformer=transformer,
    vae=vae,
    # ... otros componentes
)
```

### 3. Reemplaza la Carga por `load_gguf_pipeline`

**C√≥digo NUEVO (cargando con .gguf):**
```python
from gguf_loader import load_gguf_pipeline
from gguf_loader.scripts.run_gguf_t2v import FlowUniPCMultistepScheduler

# Configura tu scheduler
scheduler = FlowUniPCMultistepScheduler(...)

print("Cargando pipeline completo usando GGUF Loader...")
pipe = load_gguf_pipeline(
    gguf_path="ruta/a/tu/modelo.gguf",
    base_model_path="Wan-AI/Wan2.1-T2V-14B-Diffusers",
    config_path="ruta/a/tu/config.yaml",
    scheduler=scheduler
)

print("¬°Pipeline listo para usar con GGUF!")
```

¬°Eso es todo! El pipeline ahora utilizar√° el Transformer optimizado para GGUF.

## Uso Avanzado: Control Granular

Si prefieres cargar el VAE y otros componentes por tu cuenta y solo usar el m√≥dulo para 
el Transformer, utiliza `load_gguf_transformer`:

```python
from gguf_loader import load_gguf_transformer
from diffusers import WanPipeline, AutoencoderKL

# 1. Carga tus componentes como siempre
vae = AutoencoderKL.from_pretrained(
    "Wan-AI/Wan2.1-T2V-14B-Diffusers", 
    subfolder="vae"
)
# ... Carga de Tokenizer, Text Encoder, Scheduler ...

# 2. Carga SOLO el Transformer usando el m√≥dulo GGUF
gguf_transformer = load_gguf_transformer(
    gguf_path="ruta/a/tu/modelo.gguf"
)

# 3. Ensambla el pipeline manualmente
pipe = WanPipeline(
    transformer=gguf_transformer,
    vae=vae,
    # ... el resto de tus componentes
)
```

Para un ejemplo completo con gesti√≥n inteligente de memoria (CPU/GPU swap), consulta 
el script `examples/run_gguf_t2v.py`.

## Modelos GGUF Compatibles

Puedes encontrar modelos Wan en formato GGUF en los siguientes repositorios de 
Hugging Face:

- [city96](https://huggingface.co/city96) - Modelos originales y conversiones
- [QuantStack](https://huggingface.co/QuantStack) - Incluye el modelo LightX2V destilado
- [Kijai](https://huggingface.co/Kijai) - Variantes adicionales

**Modelo recomendado:** 
[Wan2.1_T2V_14B_LightX2V_StepCfgDistill_VACE-GGUF](https://huggingface.co/QuantStack/Wan2.1_T2V_14B_LightX2V_StepCfgDistill_VACE-GGUF)

### ‚ö†Ô∏è Importante sobre el Sampler

**¬°ATENCI√ìN!** Es fundamental utilizar el sampler **UniPC (Flow_Unipc)** cuando 
trabajes con modelos Wan 2.1 o 2.2. No uses otros samplers ya que pueden producir:

- Ruido en el v√≠deo resultante
- Problemas de interpretaci√≥n de latentes por parte del VAE

## Preguntas Frecuentes

**¬øQu√© ventajas tiene el formato .gguf respecto a .safetensors?**
- Mejor eficiencia en la carga (hasta 10x m√°s r√°pido)
- Menor consumo de memoria (reducci√≥n del 70%)
- Soporte para cuantizaci√≥n avanzada
- Menor dependencia de librer√≠as externas

**¬øEs compatible con todos los modelos Wan?**
Solo es compatible con modelos entrenados o convertidos al formato `.gguf`. 
Actualmente soporta Wan 2.1 y 2.2.

**¬øD√≥nde puedo consultar ejemplos de integraci√≥n?**
Consulta la carpeta `examples/` para ejemplos completos de integraci√≥n con Wan 2.1.

**¬øQu√© GPU m√≠nima necesito?**
Con GGUF puedes ejecutar modelos Wan-14B en GPUs con al menos 12GB de VRAM 
(como RTX 3060 12GB, RTX 4070, etc.).

## Licencia

Este proyecto se distribuye bajo la **licencia Apache 2.0**. Todos los componentes 
utilizados est√°n bajo la misma licencia.

## Agradecimientos

Este trabajo est√° inspirado y adaptado del excelente proyecto 
[ComfyUI-GGUF](https://github.com/city96/ComfyUI-GGUF/) de 
[@city96](https://github.com/city96). Agradecemos enormemente su contribuci√≥n a la 
comunidad de c√≥digo abierto.

## Soporte

Si tienes cualquier duda o necesitas soporte, abre una **issue** en este repositorio 
o contacta con [@acaimari](https://github.com/acaimari).
